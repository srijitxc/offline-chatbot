ğŸ¤– Local Offline AI ChatbotA privacy-first, fully offline chatbot application built with Python, Streamlit, and Ollama. This app allows you to chat with Large Language Models (LLMs) without an internet connection.âœ¨ Features100% Offline: No data leaves your machine. Perfect for secure or remote environments.Chat Memory: Remembers the context of your current conversation.Fast & Lightweight: Optimized to run on standard consumer hardware using the Llama 3.2 1B model.Clean UI: Simple web-based interface via Streamlit.ğŸ› ï¸ PrerequisitesBefore running the app, ensure you have the following installed:Ollama: Download herePython 3.10+: Download here (Ensure "Add to PATH" is checked during install).ğŸš€ Setup Instructions1. Download the AI ModelWhile online, open your terminal and pull the model "brain":Bashollama pull llama3.2:1b
2. Install DependenciesInstall the required Python libraries:Bashpip install ollama streamlit
3. Run the ApplicationNavigate to your project folder and launch the app using the Streamlit module:Bashpython -m streamlit run Untitled-1.py
ğŸ’» Hardware RequirementsComponentMinimumRecommendedRAM8 GB16 GB+CPU4 Cores8 Cores+GPUNot RequiredNVIDIA RTX or Apple M-Series (M1/M2/M3)ğŸ”§ Troubleshooting'pip' or 'streamlit' not recognized: Use python -m pip or python -m streamlit to bypass Windows Path issues.Model not found: Ensure the model name in your code (llama3.2:1b) matches exactly what you downloaded in Ollama.Slow responses: If the bot is slow, close other heavy apps (like Chrome or Games) to free up RAM.ğŸ“œ LicenseThis project is open-source and free to use.
